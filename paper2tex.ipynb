{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join('ScanSSD'))\n",
    "sys.path.insert(0, os.path.join('ScanSSD', 'layers'))\n",
    "sys.path.insert(0, os.path.join('ScanSSD', 'gtdb'))\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import nms\n",
    "\n",
    "from pix2tex import cli as pix2tex\n",
    "from PIL import Image\n",
    "\n",
    "from ScanSSD.ssd import build_ssd\n",
    "from ScanSSD.data import config\n",
    "\n",
    "from p2l_utils import get_rolling_crops, postprocess\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a paper and convert it to a list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './demo/paper.pdf'\n",
    "images_list = convert_from_path(test_path)\n",
    "print(f\"The number of pages in the pdf is {len(images_list)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LaTeX-OCR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pix2tex.LatexOCR()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ScanSSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scanssd_wrapper import *\n",
    "\n",
    "class ArgStub():\n",
    "    def __init__ (self):\n",
    "        self.cuda = True if torch.cuda.is_available() else False\n",
    "        self.kernel = (1, 5)\n",
    "        self.padding = (0, 2)\n",
    "        self.phase = 'test'\n",
    "        self.visual_threshold = 0.8\n",
    "        self.verbose = False\n",
    "        self.exp_name = 'SSD'\n",
    "        self.model_type = 512\n",
    "        self.use_char_info = False\n",
    "        self.limit = -1\n",
    "        self.cfg = 'hboxes512'\n",
    "        self.batch_size = 32\n",
    "        self.num_workers = 4\n",
    "        self.neg_mining = True\n",
    "        self.log_dir = 'logs'\n",
    "        self.stride = 0.1\n",
    "        self.window = 1200\n",
    "\n",
    "\n",
    "md = MathDetector('./saved_models/AMATH512_e1GTDB.pth', ArgStub())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images = []\n",
    "\n",
    "for temp_image in images_list:\n",
    "    img_size = 1280\n",
    "    # convert image to numpy array\n",
    "    temp_image = np.array(temp_image)\n",
    "    img = cv2.resize(temp_image, (img_size, int(img_size * temp_image.shape[0] / temp_image.shape[1])))\n",
    "    new_images.append(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform equation detection and recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "batch_size = 32\n",
    "threshold = 0.9\n",
    "iou = 0.1\n",
    "\n",
    "for idx, temp_image in enumerate(new_images):\n",
    "    crops_list, padded_crops_list, crops_info_list = get_rolling_crops(temp_image, stride=[128, 128])\n",
    "\n",
    "    scores_list = []\n",
    "    wb_list = []\n",
    "    for i in tqdm(range(0, len(padded_crops_list), batch_size)):\n",
    "        batch = padded_crops_list[i:i+batch_size]\n",
    "        window_borders, scores = md.DetectAny(batch, threshold)\n",
    "        scores_list.extend(scores)\n",
    "        wb_list.extend(window_borders)\n",
    "\n",
    "    # change crops to original image coordinates\n",
    "    bb_list, s_list = postprocess(wb_list, scores_list, crops_info_list)\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    bb_torch = torch.tensor(bb_list).float()\n",
    "    scores_torch = torch.tensor(s_list)\n",
    "\n",
    "    # perform non-maximum suppression\n",
    "    # check if bb_torch is empty\n",
    "    if bb_torch.shape[0] == 0:\n",
    "        res.append(([], []))\n",
    "        continue\n",
    "    indices = nms(bb_torch, scores_torch, iou)\n",
    "\n",
    "    bb_torch = bb_torch[indices]\n",
    "    new_bb_list = bb_torch.int().tolist()\n",
    "    new_s_list = scores_torch[indices].tolist()\n",
    "\n",
    "    res.append((new_bb_list, new_s_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Save detected equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a directory to store the results\n",
    "path2save = test_path.split('.pdf')[0]\n",
    "\n",
    "if not os.path.exists(path2save):\n",
    "    os.mkdir(path2save)\n",
    "\n",
    "for idx, temp_image in enumerate(new_images):\n",
    "    img_c = np.copy(temp_image)\n",
    "    bb_list, score_list = res[idx]\n",
    "    for i, (bb, score) in enumerate(zip(bb_list, score_list)):\n",
    "        # draw bounding boxes\n",
    "        cv2.rectangle(img_c, (bb[0], bb[1]), (bb[2], bb[3]), (0, 255, 255), 2)\n",
    "        # put the index of the bounding boxes\n",
    "        cv2.putText(img_c, str(i), (bb[0], bb[1]), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "        # put the score of the bounding boxes\n",
    "        cv2.putText(img_c, str(round(score, 2)), (bb[2], bb[1]), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imwrite(os.path.join(path2save, f'{idx}.png'), img_c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert equations to LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img_crops = []\n",
    "\n",
    "for idx, temp_image in enumerate(new_images):\n",
    "    bb_list, score_list = res[idx]\n",
    "    temp = []\n",
    "    for i in range(len(bb_list)):\n",
    "        img_c = np.copy(temp_image)\n",
    "        temp_bb = bb_list[i][:]\n",
    "\n",
    "        temp_bb[0] = max(0, temp_bb[0] - int(0.05 * (temp_bb[2] - temp_bb[0])))\n",
    "        temp_bb[1] = max(0, temp_bb[1] - int(0.05 * (temp_bb[3] - temp_bb[1])))\n",
    "        temp_bb[2] = min(img_c.shape[1], temp_bb[2] + int(0.05 * (temp_bb[2] - temp_bb[0])))\n",
    "        temp_bb[3] = min(img_c.shape[0], temp_bb[3] + int(0.05 * (temp_bb[3] - temp_bb[1])))       \n",
    "\n",
    "        # convert to int\n",
    "        temp_bb = [int(x) for x in temp_bb]\n",
    "\n",
    "        # crop the image\n",
    "        cropped_img = img_c[temp_bb[1]:temp_bb[3], temp_bb[0]:temp_bb[2]]\n",
    "\n",
    "        # resize the image to height 128\n",
    "        cropped_img = cv2.resize(cropped_img, (int(128 * cropped_img.shape[1] / cropped_img.shape[0]), 128))\n",
    "\n",
    "        # convert to PIL image\n",
    "        cropped_img = Image.fromarray(cropped_img)\n",
    "\n",
    "        temp.append(cropped_img)\n",
    "    final_img_crops.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for cropped_img_list in tqdm(final_img_crops):\n",
    "    temp_res = []\n",
    "    for img in cropped_img_list:\n",
    "        temp = model(img)\n",
    "        temp_res.append(temp)\n",
    "    results.append(temp_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
